\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{marvosym}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref} 


\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm




\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\yi}{y_i}
\newcommand{\vxj}{{\bf x}_j}
\newcommand{\vxn}{{\bf x}_n}
\newcommand{\yj}{y_j}
\newcommand{\ai}{\alpha_i}
\newcommand{\aj}{\alpha_j}
\newcommand{\X}{{\bf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\vz}{{\bf z}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\vmuk}{{\bf \mu}_k}
\newcommand{\msigmak}{{\bf \Sigma}_k}
\newcommand{\vmuj}{{\bf \mu}_j}
\newcommand{\msigmaj}{{\bf \Sigma}_j}
\newcommand{\pij}{\pi_j}
\newcommand{\pik}{\pi_k}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\yh}{\hat{y}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\newcommand{\alphai}{\alpha_i}

\pagestyle{myheadings} 
\markboth{Homework 1}{Fall 2015 CS 475 Machine Learning: Homework 1} 


\title{CS 475 Machine Learning: Homework 1\\Learning Foundations\\
\Large{Due: Monday September 14, 2015, 11:59pm}\\
50 Points Total \hspace{1cm} Version 1.0}
\author{}
\date{}

\begin{document}
\large
\maketitle
\thispagestyle{headings}

\vspace{-.5in}

{\bf Make sure to read from start to finish before beginning the assignment.}
\section{Introduction}
The goal of the programming homeworks in this course is to build a machine learning library. In each homework assignment you will expand your learning library by implementing and evaluating new algorithms. Most, but not all, programming assignments will build upon previous assignments by comparing algorithms on common data. The purpose of the first assignment will be to introduce the data and build the foundations of the learning library. 

\subsection{Components of Assignments}
Assignments consist of two parts.
\begin{enumerate}
\item {\bf Programming:} You will implement learning algorithms and test them on provided data.
\item {\bf Analytical questions:} These questions will ask you to consider questions related to the topics covered by the assignment. You will be able to answer these questions without relying on your programming.
\end{enumerate}

Assignments are worth various points (usually between 50 and 100) and the point totals will be indicated in the assignment.

Each assignment will contain a version number at the top. While we try to ensure every homework is perfect when we release it, small errors do happen. When we correct these, we'll update the version number, post a new PDF and announce the change. Each homework starts at version 1.0 (no beta).

\section{Data}
The first part of the semester will focus on supervised classification. We consider several real world binary classification datasets taken from a range of applications. Each dataset is in the same format (described below) and contains a train, development and test file. You will train your algorithm on the train file and use the development set to test that your algorithm works. The test file contains unlabeled examples that we will use to test your algorithm. It is {\bf a very good idea} to run on the test data just to make sure your code doesn't crash. You'd be surprised how often this happens.

\subsection{Biology}
	Biological research produces large amounts of data to analyze. Applications of machine learning to biology include finding regions of DNA that encode for proteins, classification of gene expression data and inferring regulatory networks from mRNA and proteomic data.
	
	Our biology task of characterizing gene splice junction sequences comes from molecular biology, a field interested in the relationships of DNA, RNA, and proteins. Splice junctions are points on a sequence at which ``superfluous'' RNA is removed before the process of protein creation in higher organisms. Exons are nucleotide sequences that are retained after splicing while introns are spliced out. The goal of this prediction task is to recognize DNA sequences that contain boundaries between exons and introns. Sequences contain exon/intron (EI) boundaries, intron/exon (IE) boundaries, or do not contain splice examples.
		
	For a binary task, you will classify sequences as either EI boundaries (label 1) or non-splice sequences (label 0). Each learning instance contains a 60 base pair sequence (ex. ACGT), with some ambiguous slots. Features encode which base pair occurs at each position of the sequence.

\subsection{Finance}
Finance is a data rich field that employs numerous statistical methods for modeling and prediction, including the modeling of financial systems and portfolios.\footnote{For an overview of such applications, see the proceedings of the 2005 NIPS workshop on machine learning in finance. \href{http://www.icsi.berkeley.edu/~moody/MLFinance2005.htm}{http://www.icsi.berkeley.edu/\char`\~moody/MLFinance2005.htm}}

Our financial task is to predict which Australian credit card applications should be accepted (label 1) or rejected (label 0). Each example represents a credit card application, where all values and attributes have been anonymized for confidentiality. Features are a mix of continuous and discrete attributes and discrete attributes have been binarized.

\subsection{NLP}
Natural language processing studies the processing and understanding of human languages. Machine learning is widely used in NLP tasks, including document understanding, information extraction, machine translation and document classification.

Our NLP task is sentiment classification. Each example is a product review taken from Amazon kitchen appliance reviews. The review is either positive (label 1) or negative (label 0) towards the product. Reviews are represented as uni-gram and bi-grams; each one and two word phrase is extracted as a feature.

\subsection{Speech}
Statistical speech processing has its roots in the 1980s and has been the focus of machine learning research for decades. The area deals with all aspects of processing speech signals, including speech transcription, speaker identification and speech information retrieval.

Our speech task is spoken letter identification. Each example comes from a speaker saying one of the twenty-six letters of English alphabet. Our goal is to predict which letter was spoken. The data was collected by asking 150 subjects to speak each letter of the alphabet twice.

Each spoken utterance is represented as a collection of 617 real valued attributes scaled to be between -1.0 and 1.0. Features include spectral coefficients; contour features, sonorant features, pre-sonorant features, and post-sonorant features. The binary task is to distinguish between the letter M (label 0) and N (label 1).

\subsection{Vision}
Computer vision processes and analyzes images and videos and it is one of the fundamental areas of robotics. Machine learning applications include identifying objects in images, segmenting video and understanding scenes in film.

Our vision task is image segmentation. In image segmentation, an image is divided into segments are labeled according to content. The images in our data have been divided into 3x3 regions. Each example is a region and features include the centroids of parts of the image, pixels in a region, contrast, intensity, color, saturation and hue. The goal is to identify the primary element in the image as either a brickface, sky, foliage, cement, window, path or grass. In the binary task, you will distinguish segments of foliage (label 0) from grass (label 1).

\subsection{Synthetic Data}
When developing algorithms it is often helpful to consider data with known properties. We typically create synthetic data for this purpose. To help test your algorithms, we are providing two synthetic datasets. These data are to help development.

\subsubsection{Easy}
The easy data is labeled using a trivial classification function. Any reasonable learning algorithm should achieve near flawless accuracy. Each example is a 10 dimensional instance drawn from a multi-variate Gaussian distribution with 0 mean and a diagonal identity covariance matrix. Each example is labeled according to the presence one of 6 features; the remaining features are noise.

\subsubsection{Hard}
Examples in this data are randomly labeled. Since there is no pattern, no learning algorithm should achieve accuracy significantly different from random guessing (50\%). Data is generated in an identical manner as \emph{Easy} except there are 94 noisy features.






\section{Programming (35 Points)}
In this assignment you will build your learning framework by writing some simple binary classification algorithms. We have provided Java code that performs most basic operations for the learning library. You will fill in the details. Search for comments that begin with {\tt TODO}; these sections need to be written. You may change the internal code as you see fit but \textbf{the behavior for the given command lines cannot be changed. Do not change the name or package of any of the provided code.}

\subsection{How to Run the Library}
The library operates in two modes: train and test. Both stages are in the main method of {\tt Classify}.

The command line for train mode is:
\begin{footnotesize}
\begin{verbatim}
java cs475.Classify -mode train -algorithm algorithm -model_file model_file -data train_file
\end{verbatim}
\end{footnotesize}
The {\tt mode} option indicates which mode to run (train or test). The {\tt algorithm} option indicates which training algorithm to use. Each assignment will specify the string argument for an algorithm. The {\tt data} option indicates the data file to load. Finally, the {\tt model\_file} option specifies where to save the trained model.

The test mode is run in a similar manner:
\begin{footnotesize}
\begin{verbatim}
java cs475.Classify -mode test -model_file model_file -data test_file -predictions_file predictions_file
\end{verbatim}
\end{footnotesize}
The {\tt model\_file} is loaded and run on the {\tt data}. Results are saved to the {\tt predictions\_file}.

As an example, the following trains an even/odd classifier on the speech training data:
\begin{footnotesize}
\begin{verbatim}
java cs475.Classify -mode train -algorithm even_odd -model_file speech.even_odd.model \
                     -data speech.train
\end{verbatim}
\end{footnotesize}
To run the trained model on development data:
\begin{footnotesize}
\begin{verbatim}
java cs475.Classify -mode test -model_file speech.even_odd.model -data speech.dev \
                     -predictions_file speech.dev.predictions
\end{verbatim}
\end{footnotesize}

As we add new algorithms we will also add command line flags to specify algorithmic parameters. These will be specified in each assignment.

To run the code you will need both {\tt library.jar} file and Apache Commons CLI (version 1.0 or above) \footnote{\href{http://commons.apache.org/proper/commons-cli/}{http://commons.apache.org/proper/commons-cli/}}. Make sure both of these jar files are on the classpath. Note that {\tt library.jar} is not an executable jar file. You need to reference it in your classpath.



\subsection{Data Formats}
The data are provided in what is commonly known as SVM-light format. Each line contains a single example:
\begin{footnotesize}
\begin{verbatim}
0 1:-0.2970 2:0.2092 5:0.3348 9:0.3892 25:0.7532 78:0.7280
\end{verbatim}
\end{footnotesize}
The first entry on the line is the label. The label can be an integer (0/1 for binary classification) or a real valued number (for regression.) The classification label of $-1$ indicates unlabeled. Subsequent entries on the line are features. The entry {\tt 25:0.7532} means that feature $25$ has value $0.7532$. Features are 1-indexed.

Model predictions are saved as one predicted label per line in the same order as the input data. The code that generates these predictions is provided in the library. The script {\tt compute\_accuracy.py} can be used to evaluate the accuracy of your predictions for classification:
\begin{footnotesize}
\begin{verbatim}
compute_accuracy.py data_file predictions_file
\end{verbatim}
\end{footnotesize}

This should give you the same results are your {\tt AccuracyEvaluator} (see below.) We provide this script since it is exactly how we will evaluate your
output. The {\tt AccuracyEvaluator} is for your benefit and convenience.

\subsection{Components}
The foundations of the learning framework have been provided for you in {\tt library.jar}. You will need to complete this library by filling in code where you see a {\tt TODO} comment. You are free to make changes to the code as needed provided you do not change the behavior of the command lines described above.

The classes of interest in the library are:
\begin{itemize}
\item {\bf FeatureVector}- The data representing an instance are stored as a feature vector. A feature vector is a vector of doubles, where the value of the $i$th dimension of the feature vector corresponds to the value of the $i$th feature. A FeatureVector must support operations such as {\tt get(index)}, which returns the value of the feature at {\tt index} and {\tt add(index, value)}, which sets the value of the feature at {\tt index}.

Since many learning applications encode instances as sparse vectors, {\tt FeatureVector} should be {\bf a sparse vector}. A sparse vector efficiently encodes very high dimensional data by not maintaining values for features with 0 values. Some common implementations of sparse vectors include hash maps and lists of index/value pairs. If you fail to do this correctly, your code will run very slowly. You will need to add a method(s) to iterate over the non-empty positions of the vector. How you chose to do this is up to you.

\item {\bf Label}- A label object encodes the label for a learning example. The {\tt Label} class is abstract and you will implement a {\tt ClassificationLabel}, which contains an int to indicate a class (binary prediction will be 0 or 1) and a {\tt RegressionLabel}, which contains a double to indicate the value of the label. {\tt toString()} methods must be written for each label since these are called to write the label to the predictions file.

\item {\bf Instance}- An instance represents a single learning example. An instance contains a data object and a label. The data object is a {\tt FeatureVector} and the label will be a {\tt Label} object. For classification, the label will be a {\tt ClassificationLabel} object. When the label is unknown (test data) the label will be null.

\item {\bf Evaluator}- This is an abstract parent class for evaluation methods. For classification you will need an {\tt AccuracyEvaluator} (not implemented.)
These should be used to evaluate your model on the train and development data. You will need to create {\tt AccuracyEvaluator} in this assignment to
evaluate the classifiers you are implementing (see below.)

\item {\bf DataReader}- This class reads in data and creates {\tt Instance} objects.

\item {\bf Predictor}- This is an abstract class that will be the parent class for all learning algorithms. Learning algorithms must implement the {\tt train} and {\tt predict} methods. Predictors must be serializable so that they can be saved after training.

\item {\bf PredictionsWriter}- Writes the predictions to a file.

\item {\bf Classify}- The main method used to run the learning library. This is where {\tt Predictor} objects are created and trained.

\end{itemize}

For those new to Java, you may find these classes helpful: HashMap, HashSet, ArrayList. Also, if you encounter serialization errors on this or future assignments, simply add {\tt implements Serializable} to the classes that are throwing the error.

\subsection{Learning Algorithms}
You will test your learning library by writing some simple learning algorithms.
\paragraph{Majority Classifier:} A majority classifier labels every object with the most common label in the training data. When two labels are tied for occurring the most often then the majority classifier picks label ``1''. This classifier should be selected by passing the string {\tt majority} as the argument for {\tt algorithm.}

\paragraph{Even/Odd Classifier:} Compute two sums: {\tt even-sum} and {\tt odd-sum}. {\tt even-sum} is the sum of the values for every even numbered feature(2nd, 4th, etc; features are 1-based so there is no 0-th feature). {\tt odd-sum} is the sum of the values for every odd numbered feature. If {\tt even-sum} $\ge$ {\tt odd-sum}, predict 1. Otherwise predict 0. We realize that this doesn't make much sense as a prediction rule. The goal is to demonstrate that you can successfully store and access the features. Remember, you should be using sparse feature vectors as described above. This classifier should be selected by passing the string {\tt even\_odd} as the argument for {\tt algorithm.}

Note that for this assignment, the algorithms are not expected to do very well on any data, including the \emph{Easy} synthetic data.

\subsection{Maximum Likelihood}
This problem will use different code from above. We have provided you with a class {\tt BernoulliLikelihood} with a main method. For this part of the assignment you
will finish writing this class.

You will be given a file containing a data sequence sampled from $n$ Bernoulli trials that represent a biased coin. If we toss our coin, it has a probability $\mu$ of heads and $1-\mu$ of tails. The random variable $x$ takes the value $1$ (heads) and $0$ (tails). The likelihood of one coin flip is:
\begin{eqnarray}
P(x|\mu) = \mu^x (1-\mu)^{1-x}
\end{eqnarray}

We have provided you with data files named coin\_flip\_n, for various $n$. Each file contains a sequence of coin flips (0/1) sampled IID, one value per line. For each file, we used
a different biased coin, which has its own value for $\mu$. Your job is to use maximum likelihood estimation to compute the value of $\mu$ for the given sequence.
Specifically, you will output two values:
\begin{enumerate}
\item The maximum likelihood solution for $\mu$.
\item The log-likelihood of the data given the maximum likelihood estimate of $\mu$.
\end{enumerate}

To do this you will complete the class {\tt BernoulliLikelihood}. The main method in this class takes the following command line arguments:

-{\tt data} a file where each line is 1 or 0. 1 is heads, 0 is tails.

You need to implement two methods:
\begin{enumerate}
\item {\tt computeMaximumLikelihood}: compute the maximum likelihood solution given the data
\item {\tt computeLogLikelihood}: compute the log likelihood of the data for the given parameter.
\end{enumerate}

You only need to modify these two methods. Nothing else. In your code, use {\tt Math.log} to ensure consistency across
assignments.

To run the code on the data:
\begin{footnotesize}
\begin{verbatim}
java cs475.BernoulliLikelihood -data filename
\end{verbatim}
\end{footnotesize}


\subsection{How We Grade Your Programs}
The programming section of your assignment will be graded using an automated grading program. Your code will be run using
the provided command line options, as well as other variations on these options (different parameters, data sets, etc.) The grader will consider the following aspects of your code.
\begin{enumerate}
\item {\bf Compilation:} Does your code compile? If not, you'll fail the programming part of the assignment. Make sure your code compiles and that you submit all of your code in the proper Java directory structure (i.e., folders for each package.) Note that you {\bf cannot} use external libraries except those given as part of the assignment.
\item {\bf Exceptions:} Does your code run without crashing?
\item {\bf Output:} Many assignments will ask you to write some data to the console. Make sure you follow the provided output instructions exactly.
\item {\bf Accuracy:} If your code works correctly, then it should achieve a certain accuracy on each data set. While there are small difference that can arise, a correctly working implementation will get the right answer.
\item {\bf Speed/Memory:} Efficiency largely doesn't matter, except where lack of efficiency severely slows your code (think so slow that we assume it is broken) or the lack of efficiency demonstrates a lack of understanding of the algorithm.  For example, if your code runs in two minutes and everyone else runs in 2 seconds, you'll lose points. Alternatively, if you require 2 gigs of memory, and everyone else needs 10 MB, you'll lose points. In general, this happens not when you are not optimizing your code, but when you've implemented something incorrectly.

\end{enumerate}

\subsection{Submitting Code}
All of the code you submit must be in a file called library.zip. We assume that the root of this zip file corresponds to the default package. Do not place code in a sub directory called src. We will assume that code in the src directory is in the package ``src'' and it won't compile (unless you actually place it in the package src). To compile your code, we will extract the contents in library.zip and compile everything that ends in .java. The java compiler assumes that directories correspond to packages. Since we require you to submit a file called cs475.Classify then you must have a directory called cs475 that contains a file named Classify.java.

It is probably easiest if all of the code you write is in the package cs475, although you can (and probably should) create sub packages like cs475.classification. You also have the option of putting code in the default package, the root of the .zip file or anywhere else you want. The key requirements are:
1) Your code must compile assuming that the root of the zip is the default package.
2) You must have a directory called cs475 that contains a file named Classify.java.

Do not submit the commons cli library or any other external libraries. Only submit  the code that you wrote and files in the cs475 package that we provided, even if you did not modify them.

\subsection{Code Readability and Style}
In general, we do not care about code style or if it conforms to Java standards for naming variables, methods, etc. However, your code should be readable, which means comments and clear organization. If your code works perfectly then you will get full credit. However, if it does not we will look at your code to determine how to allocate partial credit. If we cannot read your code or understand it, then it is very difficult to assign partial credit. Therefore, it is in your own interests to make sure that your code is reasonably readable and clear.

\subsection{Code Structure}
Your code must support the command line options and the example commands listed in the assignment. Aside from this, you are free to change the internal structure of the code, write new classes, change methods, add exception handling, etc. However, do not change the name or packages of any code you have been provided. We suggest you remember the need for clarity in your code organization.


\subsection{Knowing Your Code Works}
How do you know your code really works? That is a very difficult problem to solve. Here are a few tips:
\begin{enumerate}
\item Check results on {\tt easy} and {\tt hard}. They should be close to 100\% and 50\% respectively. For this assignment though, that won't be the case. However, you can easily check if things are working by hand.
\item Use the bulletin board. While you {\bf cannot} share code, you can share results. It is acceptable to post your results on dev data for your different algorithms. A common result will quickly emerge that you can measure against.
\item Output intermediate steps. Looking at final predictions that are wrong tells you little. Instead, print output as you go and check it to make sure it looks right. This can also be helpful when sharing information on the bulletin board.
\item Debug. If you don't know how to use the Java Debugger, learn. It's incredibly helpful.
\end{enumerate}

\subsection{Debugging}
The most common question we receive is ``how do I debug my code?'' The truth is that machine learning algorithms are very hard to debug because
the behavior of the algorithm is unknown. In these assignments, you won't know ahead of time what accuracy is expected for your algorithm on a dataset.
This is the reality of machine learning development, though in this class you have the advantage of having classmates, who may post the output of their code to the
bulletin board. While debugging machine learning code is harder, the same principles of debugging apply. Write tests for different parts of your code
to make sure it works as expected. Test it out on the easy datasets to verify it works and, when it doesn't, debug those datasets carefully. Work out on paper
the correct answer and make sure your code matches. Don't be afraid of writing your own data for specific algorithms as needed to test out different methods.
This process is part of learning machine learning algorithms and a reality of developing machine learning programs.









\section{Analytical (15 Points)}
In addition to completing the analytical questions, your assignment for this homework is to learn Latex. All homework writeups must be PDFs compiled from Latex. Why learn latex?
\begin{enumerate}
\item It is incredibly useful for writing mathematical expressions.
\item It makes references simple.
\item Many academic papers are written in latex.
\end{enumerate}
The list goes on. Additionally, it makes your assignments much easier to read than if you try to scan them in or complete them in Word.

We realize learning latex can be daunting. Fear not. There are many tutorials on the Web to help you learn. We recommend using pdflatex. It's available for nearly every operating system. Additionally, we have provided you with the tex source for this PDF, which means you can start your writeup by erasing much of the content of this writeup and filling in your answers. You can even copy and paste the few mathematical expressions in this assignment for your convenience. As the semester progresses, you'll no doubt become more familiar with latex, and even begin to appreciate using it.

Be sure to check out this cool latex tool for finding symbols. It uses machine learning! \url{http://detexify.kirelabs.org/classify.html}


\paragraph{Some Hints:} The following problems require you to know some basic concepts in probability and linear algebra. You are encouraged to google these basic concepts.

\paragraph{1 (4 points)} Suppose you are given a biased coin. Specifically, if you flip the coin, it comes up heads with probability $5/6$, and comes up tails with probability $1/6$.  What is the expected number of flips such that the coin comes up tails exactly once.

\paragraph{2 (3 points)} Suppose a family have two children. Given the fact that one is a girl, what is the probably that the other child is also a girl?

\paragraph{3 (4 points)} Please show that the following loss functions are convex: (a) $\ell (y, \hat{y}) = |y-\hat{y}|$; (b) $\ell (y, \hat{y}) = (y-\hat{y})^2$.

%\paragraph{4 (2 points)} Given a positive semidefinite symmetric matrix $A\in\mathbb{R}^{n \times n}$, please show $\sum_{i=1}^n A_{ii} = \sum_{i=1}^n\lambda_i$, where $\lambda_1$,...,$\lambda_n$ are eigenvalues of $A$.

%\paragraph{5 (3 points)} Please derive $\nabla\log \det(X)=\left[\frac{\partial\log \det(X)}{\partial X_{ij}}\right]_{i,j=1}^n$, where $X$ is a $n$ by $n$, symmetric, and positive definite matrix.

\paragraph{4 (3 points)} 
Give an example of an optimal hypothesis, a finite hypothesis class that contains the optimal hypothesis, and an infinite class that does not contain the optimal hypothesis.

\paragraph{5 (1 point)}
How are you comfortable with probability and linear algebra?

\section{What to Submit}
In each assignment you will submit two things.
\begin{enumerate}
\item {\bf Code:} Your code as a zip file named {\tt library.zip}. {\bf You must submit source code (.java files)}. We will run your code using the exact command lines described above, so make sure it works ahead of time. Remember to submit all of the source code, including what we have provided to you.
\item {\bf Writeup:} Your writeup as a {\bf PDF file} (compiled from latex) containing answers to the analytical questions asked in the assignment. Make sure to include your name in the writeup PDF and use the provided latex template for your answers.
\end{enumerate}
Make sure you name each of the files exactly as specified (library.zip and writeup.pdf).

To submit your assignment, visit the ``Homework'' section of the website (\href{http://www.cs475.org/}{http://www.cs475.org/}.)

\section{Questions?}
Remember to submit questions about the assignment to the appropriate group on the class discussion board: \href{http://bb.cs475.org/}{http://bb.cs475.org}.

\end{document}
